{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c22413e2-8890-4722-aed3-ebb170419bad",
   "metadata": {},
   "source": [
    "### Main Task - Scrape Key Events Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd13fc47-ee46-4933-ae01-5089ef0ba883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully.\n"
     ]
    }
   ],
   "source": [
    "# === 1. SETUP LIBRARIES ===\n",
    "import pandas as pd\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import matplotlib.pyplot as plt \n",
    "import os\n",
    "import logging\n",
    "\n",
    "print(\"Libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "daef1366-d323-4db1-a79f-4a9939c346f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WebDriver initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "# === 2. INITIALIZE THE WEB DRIVER ===\n",
    "# This sets up and launches a new Chrome browser window controlled by your code.\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service)\n",
    "print(\"WebDriver initialized successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e578826-d39b-4f1a-a0dd-60d463b1fd89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Navigated to: Key events of the 20th century - Wikipedia\n"
     ]
    }
   ],
   "source": [
    "# === 3. SCRAPE THE KEY EVENTS PAGE (MAIN TASK) ===\n",
    "# The specific URL your mentor confirmed\n",
    "page_url = \"https://en.wikipedia.org/wiki/Key_events_of_the_20th_century\"\n",
    "driver.get(page_url)\n",
    "print(f\"Navigated to: {driver.title}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c50b6c05-b8d6-4959-9bc0-7065b437082f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use find_element (singular) because an ID should be unique on a page\n",
    "content_element = driver.find_element(By.ID, \"mw-content-text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0f0659a-c9c4-4eac-88c1-a8acdd73f53a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully extracted text from the main content block.\n"
     ]
    }
   ],
   "source": [
    "# Get all the text from just this content block\n",
    "page_text = content_element.text\n",
    "print(\"Successfully extracted text from the main content block.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdc1556f-b103-4d62-ba56-61cab7411e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content successfully saved to '20th_century_key_events.txt'.\n"
     ]
    }
   ],
   "source": [
    "# === 4. SAVE THE SCRAPED CONTENT TO A FILE ===\n",
    "# This saves the extracted text into the .txt file as required\n",
    "filename = \"20th_century_key_events.txt\"\n",
    "with open(filename, 'w', encoding='utf-8') as f:\n",
    "    f.write(page_text)\n",
    "\n",
    "print(f\"Content successfully saved to '{filename}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24e4379-e089-4df7-8369-39d5d73e3456",
   "metadata": {},
   "source": [
    "### Bonus Task - Scrape List of Countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a67e3b2a-892f-4eb5-9a64-eda08b574403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting the Final and Correct Solution ---\n",
      "Fetching content from: https://en.wikipedia.org/wiki/List_of_sovereign_states\n",
      "Page content fetched successfully.\n",
      "\n",
      "SUCCESS: Scraped 195 countries.\n",
      "Successfully saved the final, clean list to 'countries_list.csv'.\n",
      "\n",
      "--- First 5 Countries ---\n",
      "       Country\n",
      "0  Afghanistan\n",
      "1      Albania\n",
      "2      Algeria\n",
      "3      Andorra\n",
      "4       Angola\n",
      "\n",
      "--- Last 5 Countries ---\n",
      "       Country\n",
      "190  Venezuela\n",
      "191    Vietnam\n",
      "192      Yemen\n",
      "193     Zambia\n",
      "194   Zimbabwe\n"
     ]
    }
   ],
   "source": [
    "# === 1. IMPORT THE CORRECT LIBRARIES ===\n",
    "# We are Only using Requests and BeautifulSoup.\n",
    "print(\"--- Starting the Final and Correct Solution ---\")\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# === 2. GET THE PAGE CONTENT DIRECTLY WITH REQUESTS ===\n",
    "# THIS IS THE NEW, STABLE URL. We are not using any mobile \".m.\" page.\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_sovereign_states\"\n",
    "print(f\"Fetching content from: {url}\")\n",
    "\n",
    "try:\n",
    "    page = requests.get(url)\n",
    "    page.raise_for_status()  # This will raise an error if the page doesn't load correctly.\n",
    "    print(\"Page content fetched successfully.\")\n",
    "\n",
    "    # === 3. PARSE THE HTML AND FIND THE DATA ===\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    # Find the main table, which has the class 'wikitable'.\n",
    "    # This is a stable and reliable locator on this page.\n",
    "    table = soup.find('table', class_='wikitable')\n",
    "\n",
    "    country_list = []\n",
    "    # Find all rows <tr> inside the table's body <tbody>.\n",
    "    for row in table.tbody.find_all('tr'):\n",
    "        # The country name is in the first data cell <td> of the row.\n",
    "        # We find the first 'td' tag.\n",
    "        first_cell = row.find('td')\n",
    "        \n",
    "        # If a cell exists (this skips the header row), find the link <a> inside it.\n",
    "        if first_cell:\n",
    "            link = first_cell.find('a')\n",
    "            if link:\n",
    "                country_list.append(link.get_text(strip=True))\n",
    "\n",
    "    # === 4. CREATE DATAFRAME AND SAVE ===\n",
    "    if country_list:\n",
    "        print(f\"\\nSUCCESS: Scraped {len(country_list)} countries.\")\n",
    "        countries_df = pd.DataFrame(country_list, columns=['Country'])\n",
    "        countries_df.to_csv('countries_list.csv', index=False)\n",
    "        print(\"Successfully saved the final, clean list to 'countries_list.csv'.\")\n",
    "        print(\"\\n--- First 5 Countries ---\")\n",
    "        print(countries_df.head())\n",
    "        print(\"\\n--- Last 5 Countries ---\")\n",
    "        print(countries_df.tail())\n",
    "    else:\n",
    "        print(\"\\nFAILURE: Could not scrape the country list from the table.\")\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"An error occurred while fetching the page: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:20th_century] *",
   "language": "python",
   "name": "conda-env-20th_century-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
