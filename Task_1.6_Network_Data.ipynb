{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9584dcde-3208-44cd-b95d-54cbdbcd035b",
   "metadata": {},
   "source": [
    "# Task 1.6: Building a Network Dataset with NLP\n",
    "\n",
    "### Project: 20th Century Geopolitical Interrelations\n",
    "### Author: Fariya Asghar\n",
    "### Date: 23.07.2025\n",
    "\n",
    "### Abstract\n",
    "This notebook implements the Natural Language Processing (NLP) phase of the project. This approach uses a more robust and accurate methodology to extract geopolitical relationships from the \"Key Events of the 20th Century\" text.\n",
    "\n",
    "The refined methodology is as follows:\n",
    "\n",
    "1.  **Data Loading & Normalization:** Ingest the scraped country list and the raw event text. Both the country list and the event text are converted to lowercase for consistent matching.\n",
    "2.  **Text Wrangling:** A targeted dictionary is used to standardize common adjectival forms (e.g., \"Soviet\" -> \"russia\") and abbreviations (e.g., \"u.s.\" -> \"united states\"). This manual approach is more precise than the previous automated one.\n",
    "3.  **Named Entity Recognition (NER):** The wrangled text is processed using spaCy. We then leverage spaCy's pre-trained ability to identify Geopolitical Entities (`GPE`).\n",
    "4.  **Entity Filtering:** We iterate through the sentences and their identified `GPE` entities. An entity is only kept if it exists in our master list of countries, ensuring high accuracy and eliminating false positives. This is the correct filtering method as per the task requirements.\n",
    "5.  **Relationship Extraction:** For sentences containing two or more verified countries, unique relationship pairs (edges) are created.\n",
    "6.  **Output:** The final output is a structured pandas DataFrame counting the frequency of each unique country-pair interaction, saved as `country_relationships.csv`. This file is the direct input for the network analysis in the next task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ead1b28e-89d0-4f5f-aa41-c8e6cfd1cf2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy 'en_core_web_sm' model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries and Load Data\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import itertools\n",
    "import spacy\n",
    "\n",
    "# Load the pre-trained spaCy English language model\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    print(\"spaCy 'en_core_web_sm' model loaded successfully.\")\n",
    "except OSError:\n",
    "    print(\"Model not found. Please run 'python -m spacy download en_core_web_sm'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "90f8bc30-54cf-4e33-89e2-746f75725c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded and normalized 209 countries.\n",
      "Successfully loaded the raw text from the events file.\n"
     ]
    }
   ],
   "source": [
    "# --- Load and Prepare Data ---\n",
    "\n",
    "# Load the list of countries.\n",
    "# We immediately strip whitespace and convert to lowercase for consistent matching.\n",
    "try:\n",
    "    countries_df = pd.read_csv('countries_list_20th_century_1.5.csv')\n",
    "    country_list = countries_df['country_name'].str.strip().str.lower().tolist()\n",
    "    print(f\"Successfully loaded and normalized {len(country_list)} countries.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'countries_list_20th_century_1.5.csv' not found. Please ensure the file is in the same directory.\")\n",
    "    country_list = []\n",
    "\n",
    "# Load the raw text from the events page.\n",
    "try:\n",
    "    with open('20th_century_key_events.txt', 'r', encoding='utf-8') as file: \n",
    "        raw_text = file.read()\n",
    "    print(\"Successfully loaded the raw text from the events file.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: The events text file was not found.\")\n",
    "    raw_text = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95825656-6051-48b0-bc29-ec5667eed724",
   "metadata": {},
   "source": [
    "### 2. Text Wrangling and Standardization\n",
    "\n",
    "**Observations and Plan:**\n",
    "The raw text contains many variations of country names (adjectives, abbreviations). To ensure the NER model performs optimally, we must standardize these. This approach uses a curated, manual dictionary for replacements, which is more accurate and avoids the errors (e.g., mapping \"and\" to a country). The entire text will first be converted to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "49f5c223-29e3-4b51-ad42-344e3a1a5509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying text standardizations...\n",
      "Wrangling complete. Cleaned text saved to '20th_century_wrangled_text.txt'.\n"
     ]
    }
   ],
   "source": [
    "# Create a targeted dictionary for common replacements.\n",
    "replacement_dict = {\n",
    "    \"u.s.\": \"united states\",\n",
    "    \"us\": \"united states\",\n",
    "    \"u.k.\": \"united kingdom\",\n",
    "    \"uk\": \"united kingdom\",\n",
    "    \"soviet union\": \"russia\",\n",
    "    \"soviet\": \"russia\",\n",
    "    \"german\": \"germany\",\n",
    "    \"british\": \"united kingdom\",\n",
    "    \"french\": \"france\",\n",
    "    \"italian\": \"italy\",\n",
    "    \"japanese\": \"japan\",\n",
    "    \"chinese\": \"china\",\n",
    "    \"american\": \"united states\"\n",
    "}\n",
    "\n",
    "# First, normalize the entire text to lowercase.\n",
    "normalized_text = raw_text.lower()\n",
    "\n",
    "# Apply each replacement using regular expressions with word boundaries (\\b).\n",
    "# This prevents replacing parts of words (e.g., 'us' in 'house').\n",
    "print(\"Applying text standardizations...\")\n",
    "for old, new in replacement_dict.items():\n",
    "    normalized_text = re.sub(rf'\\b{re.escape(old)}\\b', new, normalized_text)\n",
    "\n",
    "# Save the clean, wrangled version for traceability and future use.\n",
    "wrangled_filename = \"20th_century_wrangled_text.txt\"\n",
    "with open(wrangled_filename, 'w', encoding='utf-8') as f:\n",
    "    f.write(normalized_text)\n",
    "print(f\"Wrangling complete. Cleaned text saved to '{wrangled_filename}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622815e2-ea6e-4660-866e-d494182a1f6c",
   "metadata": {},
   "source": [
    "### 3. Apply NER and Filter for Country Entities\n",
    "\n",
    "Now that the text is clean, we process it with spaCy. We will then iterate through each sentence, find entities labeled as `GPE` (Geopolitical Entity), and perform the final, crucial filtering step: checking if the found entity exists in our master `country_list`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "15353865-4cc3-4e07-a0d5-b0cee6647672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering sentences for verified country names...\n",
      "Filtering complete. Found 162 sentences containing at least one country.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>country_entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>after a period of diplomatic and military esca...</td>\n",
       "      <td>[France, Germany, Russia]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the bolsheviks negotiated the treaty of brest-...</td>\n",
       "      <td>[Russia, Germany]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>in the treaty, bolshevik russia ceded the balt...</td>\n",
       "      <td>[Russia, Germany]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>it also recognized the independence of ukraine...</td>\n",
       "      <td>[United States, Germany]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>combined with already existing malnourishment ...</td>\n",
       "      <td>[Russia]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  \\\n",
       "0  after a period of diplomatic and military esca...   \n",
       "1  the bolsheviks negotiated the treaty of brest-...   \n",
       "2  in the treaty, bolshevik russia ceded the balt...   \n",
       "3  it also recognized the independence of ukraine...   \n",
       "4  combined with already existing malnourishment ...   \n",
       "\n",
       "            country_entities  \n",
       "0  [France, Germany, Russia]  \n",
       "1          [Russia, Germany]  \n",
       "2          [Russia, Germany]  \n",
       "3   [United States, Germany]  \n",
       "4                   [Russia]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Process the entire normalized text with spaCy\n",
    "doc = nlp(normalized_text)\n",
    "\n",
    "# This list will store dictionaries, each representing a sentence with verified countries.\n",
    "filtered_sentences = []\n",
    "print(\"Filtering sentences for verified country names...\")\n",
    "\n",
    "for sent in doc.sents:\n",
    "    sentence_text = sent.text.strip()\n",
    "    \n",
    "    # Use a set to store unique countries found in this sentence to avoid duplicates.\n",
    "    countries_in_sentence = set()\n",
    "    \n",
    "    # Check every entity identified by spaCy in the sentence.\n",
    "    for ent in sent.ents:\n",
    "        # We are only interested in Geopolitical Entities.\n",
    "        if ent.label_ == \"GPE\":\n",
    "            # Clean the entity text (lowercase, strip whitespace) for matching.\n",
    "            ent_clean = ent.text.strip().lower()\n",
    "            \n",
    "            # THE KEY STEP: Check if this cleaned entity is in our master country list.\n",
    "            if ent_clean in country_list:\n",
    "                # If it's a match, add the standardized, title-cased name.\n",
    "                countries_in_sentence.add(ent_clean.title())\n",
    "\n",
    "    # Only if we found at least one verified country, we add it to our results.\n",
    "    if len(countries_in_sentence) > 0:\n",
    "        filtered_sentences.append({\n",
    "            \"sentence\": sentence_text,\n",
    "            \"country_entities\": list(countries_in_sentence)\n",
    "        })\n",
    "\n",
    "# Create the final filtered DataFrame.\n",
    "filtered_sentences_df = pd.DataFrame(filtered_sentences)\n",
    "print(f\"Filtering complete. Found {len(filtered_sentences_df)} sentences containing at least one country.\")\n",
    "display(filtered_sentences_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d52632-959f-4b6c-87bc-1ab399e6bc62",
   "metadata": {},
   "source": [
    "### 4. Create and Aggregate Relationship Pairs\n",
    "\n",
    "With the correctly filtered sentences, we can now extract the relationships. We will iterate through sentences containing two or more countries and create a pair for each unique combination. Finally, we will count the frequency of each pair to determine the strength of their relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ef3ebd09-51bc-4a3a-a389-b4ab55e4089b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created a final DataFrame with 154 unique country relationships.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Germany</td>\n",
       "      <td>Russia</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Poland</td>\n",
       "      <td>Russia</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Japan</td>\n",
       "      <td>Russia</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>France</td>\n",
       "      <td>Russia</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Germany</td>\n",
       "      <td>Poland</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>France</td>\n",
       "      <td>Germany</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Japan</td>\n",
       "      <td>United States</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Germany</td>\n",
       "      <td>Italy</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>India</td>\n",
       "      <td>Pakistan</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Germany</td>\n",
       "      <td>Japan</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Egypt</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Egypt</td>\n",
       "      <td>Libya</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Finland</td>\n",
       "      <td>Russia</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Finland</td>\n",
       "      <td>Poland</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Italy</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     source          target  value\n",
       "0   Germany          Russia     12\n",
       "1    Poland          Russia      6\n",
       "2     Japan          Russia      6\n",
       "3    France          Russia      5\n",
       "4   Germany          Poland      5\n",
       "5    France         Germany      5\n",
       "6     Japan   United States      4\n",
       "7   Germany           Italy      4\n",
       "8     India        Pakistan      3\n",
       "9   Germany           Japan      3\n",
       "10    Egypt  United Kingdom      2\n",
       "11    Egypt           Libya      2\n",
       "12  Finland          Russia      2\n",
       "13  Finland          Poland      2\n",
       "14    Italy  United Kingdom      2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This list will store all raw relationship pairs, e.g., ('Germany', 'Russia').\n",
    "relationships = []\n",
    "\n",
    "# Iterate through the rows of our filtered DataFrame.\n",
    "for index, row in filtered_sentences_df.iterrows():\n",
    "    countries = row['country_entities']\n",
    "    \n",
    "    # We only care about relationships, which require at least two countries.\n",
    "    if len(countries) >= 2:\n",
    "        # itertools.combinations creates all unique pairs from the list.\n",
    "        # We sort the list first to ensure the pairs are created in a consistent order.\n",
    "        for pair in itertools.combinations(sorted(countries), 2):\n",
    "            relationships.append(pair)\n",
    "\n",
    "# Create a DataFrame from the list of pairs.\n",
    "relationships_df = pd.DataFrame(relationships, columns=[\"source\", \"target\"])\n",
    "\n",
    "# --- Count Frequencies and Finalize ---\n",
    "\n",
    "# The most efficient way to count pairs is using value_counts().\n",
    "final_relationships_df = relationships_df.value_counts().reset_index()\n",
    "final_relationships_df.columns = [\"source\", \"target\", \"value\"]\n",
    "\n",
    "print(f\"Created a final DataFrame with {len(final_relationships_df)} unique country relationships.\")\n",
    "display(final_relationships_df.head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f631f0-da32-4e13-acd5-d3b59f28dc23",
   "metadata": {},
   "source": [
    "### 5. Save Final Output\n",
    "The process is complete. The final DataFrame, containing the source country, target country, and the frequency of their co-occurrence (value), is now saved to a CSV file. This file is ready for the network visualization task in Exercise 1.7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "83e3236f-31ba-4c1b-b3d7-68a16e846ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully saved the final relationships data to 'country_relationships.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Save the final DataFrame to a CSV file.\n",
    "output_filename = \"country_relationships.csv\"\n",
    "final_relationships_df.to_csv(output_filename, index=False)\n",
    "\n",
    "print(f\"\\nSuccessfully saved the final relationships data to '{output_filename}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1925e657-8770-4435-97a6-13ddd11e911d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:20th_century] *",
   "language": "python",
   "name": "conda-env-20th_century-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
